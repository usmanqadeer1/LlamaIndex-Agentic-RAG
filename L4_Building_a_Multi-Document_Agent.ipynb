{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdfs/metagpt.pdf', 'pdfs/longlora.pdf', 'pdfs/selfrag.pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "papers = [\"pdfs/\" + p for p in papers]\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: pdfs/metagpt.pdf\n",
      "Getting tools for paper: pdfs/longlora.pdf\n",
      "Getting tools for paper: pdfs/selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff58c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2c6a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the experiments is the PG19 test split.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results indicate that the models achieve better perplexity with longer context sizes, showcasing the effectiveness of the efficient fine-tuning method. The models demonstrate improved efficiency and comparable performance to full attention or full fine-tuning baselines as the context size increases. Additionally, the results highlight the models' state-of-the-art performance on various benchmarks and tasks, emphasizing their effectiveness, generalization capabilities, and efficiency improvements compared to existing benchmarks and models.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. The evaluation results show that the models achieve better perplexity with longer context sizes, demonstrating the effectiveness of the efficient fine-tuning method. The models exhibit improved efficiency and comparable performance to full attention or full fine-tuning baselines as the context size increases. Furthermore, the results highlight the models' state-of-the-art performance on various benchmarks and tasks, emphasizing their effectiveness, generalization capabilities, and efficiency improvements compared to existing benchmarks and models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It involves training language models to learn to retrieve, generate, and critique text passages and their own generation using special tokens called reflection tokens. This framework enables the model to tailor its behavior at test time by leveraging reflection tokens, leading to significant improvements in performance, factuality, and citation accuracy compared to other models. Additionally, Self-RAG focuses on browser-assisted question-answering with a particular emphasis on evaluating the relevance, supportiveness, and usefulness of the generated responses. It uses reflection tokens to assess whether the output is fully supported by the evidence provided, whether the evidence is relevant to the task, and how useful the response is in answering the query.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is a framework that extends the context length of large language models efficiently with minimal accuracy compromise. It introduces S2-Attn during training to approximate the standard self-attention pattern, which is easy to implement and retains the original attention architecture during inference. LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling significant extension of context lengths. Additionally, LongLoRA consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components, focusing on modeling relations between facial action units and enhancing model generalization through tampering with AU-related regions. The framework has achieved state-of-the-art performance in cross-dataset and cross-manipulation evaluations, provides challenging pseudo-samples for learning, and offers qualitative visualizations of tampered regions.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It involves training language models to learn to retrieve, generate, and critique text passages and their own generation using special tokens called reflection tokens. This framework enables the model to tailor its behavior at test time by leveraging reflection tokens, leading to significant improvements in performance, factuality, and citation accuracy compared to other models. Additionally, Self-RAG focuses on browser-assisted question-answering with a particular emphasis on evaluating the relevance, supportiveness, and usefulness of the generated responses. It uses reflection tokens to assess whether the output is fully supported by the evidence provided, whether the evidence is relevant to the task, and how useful the response is in answering the query.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is a framework that extends the context length of large language models efficiently with minimal accuracy compromise. It introduces S2-Attn during training to approximate the standard self-attention pattern, which is easy to implement and retains the original attention architecture during inference. LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling significant extension of context lengths. Additionally, LongLoRA consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components, focusing on modeling relations between facial action units and enhancing model generalization through tampering with AU-related regions. The framework has achieved state-of-the-art performance in cross-dataset and cross-manipulation evaluations, provides challenging pseudo-samples for learning, and offers qualitative visualizations of tampered regions.\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It involves training language models to learn to retrieve, generate, and critique text passages and their own generation using special tokens called reflection tokens. This framework enables the model to tailor its behavior at test time by leveraging reflection tokens, leading to significant improvements in performance, factuality, and citation accuracy compared to other models. Additionally, Self-RAG focuses on browser-assisted question-answering with a particular emphasis on evaluating the relevance, supportiveness, and usefulness of the generated responses. It uses reflection tokens to assess whether the output is fully supported by the evidence provided, whether the evidence is relevant to the task, and how useful the response is in answering the query.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is a framework that extends the context length of large language models efficiently with minimal accuracy compromise. It introduces S2-Attn during training to approximate the standard self-attention pattern, which is easy to implement and retains the original attention architecture during inference. LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling significant extension of context lengths. Additionally, LongLoRA consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components, focusing on modeling relations between facial action units and enhancing model generalization through tampering with AU-related regions. The framework has achieved state-of-the-art performance in cross-dataset and cross-manipulation evaluations, provides challenging pseudo-samples for learning, and offers qualitative visualizations of tampered regions.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdfs/metagpt.pdf',\n",
       " 'pdfs/longlora.pdf',\n",
       " 'pdfs/loftq.pdf',\n",
       " 'pdfs/swebench.pdf',\n",
       " 'pdfs/selfrag.pdf',\n",
       " 'pdfs/zipformer.pdf',\n",
       " 'pdfs/values.pdf',\n",
       " 'pdfs/finetune_fair_diffusion.pdf',\n",
       " 'pdfs/knowledge_card.pdf',\n",
       " 'pdfs/metra.pdf',\n",
       " 'pdfs/vr_mcl.pdf']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]\n",
    "papers = [\"pdfs/\" + p for p in papers]\n",
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: pdfs/metagpt.pdf\n",
      "Getting tools for paper: pdfs/longlora.pdf\n",
      "Getting tools for paper: pdfs/loftq.pdf\n",
      "Getting tools for paper: pdfs/swebench.pdf\n",
      "Getting tools for paper: pdfs/selfrag.pdf\n",
      "Getting tools for paper: pdfs/zipformer.pdf\n",
      "Getting tools for paper: pdfs/values.pdf\n",
      "Getting tools for paper: pdfs/finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: pdfs/knowledge_card.pdf\n",
      "Getting tools for paper: pdfs/metra.pdf\n",
      "Getting tools for paper: pdfs/vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_values', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metra with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that are merged, resolve issues in the repository, and introduce new tests. Each task instance includes the codebase, problem statement aggregated from related issues, test patch, and gold patch. The dataset is validated through execution-based verification to ensure usability and non-triviality of the solutions. It is designed to be easily updatable with new task instances based on PRs created after the training date of language models.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that are merged, resolve issues in the repository, and introduce new tests. Each task instance includes the codebase, problem statement aggregated from related issues, test patch, and gold patch. The dataset is validated through execution-based verification to ensure usability and non-triviality of the solutions. It is designed to be easily updatable with new task instances based on PRs created after the training date of language models.\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that are merged, resolve issues in the repository, and introduce new tests. Each task instance includes the codebase, problem statement aggregated from related issues, test patch, and gold patch. The dataset is validated through execution-based verification to ensure usability and non-triviality of the solutions. It is designed to be easily updatable with new task instances based on PRs created after the training date of language models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n",
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs) with minimal accuracy compromise. It utilizes Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significant extension of context window while reducing GPU memory cost and training time compared to standard full fine-tuning. By combining improved LoRA with S2-Attn, LongLoRA achieves strong empirical results on various tasks and model sizes. Additionally, the paper focuses on context scaling through contrastive training, utilizing DeepSpeed and Flash-Attention2 to maximize context length experiments and highlighting the impact of group sizes in the S2-Attn component on fine-tuning models to different context lengths. The efficiency of LongLoRA is evaluated in comparison to full fine-tuning and plain LoRA, emphasizing the reduction in training hours while maintaining performance, along with showcasing the benefits of S2-Attn in reducing computational costs, particularly with larger context lengths.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ paper\"}\n",
      "=== Function Output ===\n",
      "The LoftQ paper introduces a novel quantization framework tailored for Large Language Models (LLMs), combining quantization and low-rank approximation to approximate high-precision pre-trained weights. This framework offers a beneficial initialization for subsequent Low-Rank Adaptation (LoRA) fine-tuning, enhancing generalization in downstream tasks. LoftQ has been tested across various tasks like natural language understanding, question answering, summarization, and natural language generation, consistently surpassing existing quantization methods, especially in challenging low-bit scenarios. The paper delves into implementation specifics and hyper-parameter configurations for training different models using LoftQ, emphasizing its compression capabilities. It also compares LoftQ with pruning methods, emphasizing its efficacy in reducing memory usage during training and storage, and extends its application to convolutional layers, demonstrating its versatility across various neural network layers.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs) with minimal accuracy compromise. It utilizes Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significant extension of the context window while reducing GPU memory cost and training time compared to standard full fine-tuning. By combining improved LoRA with S2-Attn, LongLoRA achieves strong empirical results on various tasks and model sizes. The paper focuses on context scaling through contrastive training, utilizing DeepSpeed and Flash-Attention2 to maximize context length experiments and highlighting the impact of group sizes in the S2-Attn component on fine-tuning models to different context lengths. The efficiency of LongLoRA is evaluated in comparison to full fine-tuning and plain LoRA, emphasizing the reduction in training hours while maintaining performance, along with showcasing the benefits of S2-Attn in reducing computational costs, particularly with larger context lengths.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces a novel quantization framework tailored for Large Language Models (LLMs), combining quantization and low-rank approximation to approximate high-precision pre-trained weights. This framework offers a beneficial initialization for subsequent Low-Rank Adaptation (LoRA) fine-tuning, enhancing generalization in downstream tasks. LoftQ has been tested across various tasks like natural language understanding, question answering, summarization, and natural language generation, consistently surpassing existing quantization methods, especially in challenging low-bit scenarios. The paper delves into implementation specifics and hyper-parameter configurations for training different models using LoftQ, emphasizing its compression capabilities. It also compares LoftQ with pruning methods, emphasizing its efficacy in reducing memory usage during training and storage, and extends its application to convolutional layers, demonstrating its versatility across various neural network layers.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3071c-a0dd-4546-8273-7a82c2c915f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
